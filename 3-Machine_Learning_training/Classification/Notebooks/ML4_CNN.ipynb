{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:10pt\">Robotics & AI workshop @ PPU – June 2022 – Jean-Luc Charles (Jean-Luc.charles@ensam.eu) – CC BY-SA 4.0 – v1.0</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning with tensorflow2 & keras\n",
    "\n",
    "# Train/operate a Convolutional Neural Network (CNN) for the classification of handwritten digits images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<span style=\"color:brown;font-family:arial;font-size:14pt\"> \n",
    "It is important to use a <span style=\"font-weight:bold;\">Python Virtual Environment</span> (PVE) for main Python projects: a PVE makes it possible to control for each project the versions of the Python interpreter and the \"sensitive\" modules (like tensorflow).</span></div>\n",
    "\n",
    "All the notebooks in this directory must be loaded into a `jupyter notebook` launched in the PVE <b><span style=\"color: rgb(200, 151, 102);\" >pyml</span></b> specially created for the workshop.<br>\n",
    "They must be worked in this order:\n",
    "- `ML1_MNIST.ipynb`: check that the <b><span style=\"color: rgb(200, 151, 102);\">pyml</span></b> EVP is fully operationnal, load and use the data from the MNIST database (images and labels).\n",
    "- `ML2_DNN.ipynb`: build a Dense Neural Network, train it with data from the MNIST and evaluate its performance.\n",
    "- `ML3_DNN_ipynb`: re-load a trained DNN and evaluate its performnce with MNIST test data.\n",
    "- `ML4_CNN.ipynb`: build a Convolutional Neural Network, train it with the MNIST database, ebvaluate its performance and use it with test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targeted learning objectives:\n",
    "- Learn how a convolutional neural network works.\n",
    "- Know how to build a Convolutional Neural Network (CNN) using the **tendorflow** and **keras** modules.\n",
    "- Know how to train a CNN to classify MNIST images.\n",
    "- Knowing how to use the trained network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General principles\n",
    "\n",
    "Convolutional Neural Networks (CNN) offer particularly effective structures for analyzing the content of images. For this, the CNN implement specific processing and architecture:\n",
    "- the extraction of features from images using **convolutional filters**,\n",
    "- the reduction with **pooling filters** of the amount of information generated by the numerous convolution filters,\n",
    "- an architecture that stacks \"convolution > activation > pooling...\" layers responsible for extracting the features of the image which are at the end flattened and sent as input to a Dense Neural Network (DNN) network responsible for the classification step.\n",
    "\n",
    "In the following, you will build a CNN inspired by the `LeNet5`, one of the first CNN proposed by Yann LeCun *et al.* in the 90s for the recognition of MNIST images:\n",
    "\n",
    "<p style=\"text-align:center; font-style:italic; font-size:12px;\">\n",
    "    <img src=\"img/LeNet5.png\" <br>\n",
    "    [Lecun, Y.; Bottou, L.; Bengio, Y.; Haffner, P. (1998). \"Gradient-based learning applied to document recognition\". Proceedings of the IEEE. 86 (11): 2278–2324. doi:10.1109/5.726791.]\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from an image with a convolution filter\n",
    "\n",
    "The convolution of an image by a filter (also called kernel) consists in moving a _small 2D window_ ( 3x3, 5x5 ....) aver the pixels of the image and in calculating each time the contracted tensorial product (sum of the products term by term) between the elements of the filter and the pixels of the image delimited by the window of the filter.<br>\n",
    "\n",
    "The animation below illustrates the convolution of a 5x5 image by a 3x3 filter without *padding* on the edges: we obtain a new smaller image of 3x3 pixels<br>\n",
    "<p style=\"text-align:center; font-style:italic; font-size:12px;\">\n",
    "    <img src=\"img/filter_3x3.png\" width=\"80\" style=\"display:inline-block;\">\n",
    "    <img src=\"img/Convolution_schematic.gif\" width=\"300\" style=\"display:inline-block;\"><br>\n",
    "    [image credit: <a href=\"http://deeplearning.stanford.edu/tutorial\">Stanford deep learning tutorial</A>]\n",
    "</p>\n",
    "\n",
    "To keep the size of the input image, we can use the *padding* technique to add new data on the edges of the image (by duplicating the data on the edges, or adding rows and columns of 0 ... for example) :\n",
    "\n",
    "<p style=\"text-align:center; font-style:italic; font-size:12px;\">\n",
    "    <img src=\"img/padding.gif\" width=\"350\"><br>\n",
    "    [image credit: <a href=\"https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2\">Arden Dertat</a>]\n",
    "</p>\n",
    "\n",
    "The purpose of the convolution is to extract particular features present in the source image: we call it a \"feature map\" to designate the image produced by the convolution operation. The state of the art leads to the use several convolutional filters to extract different features: one can have up to several tens of convolutional filters in the same layer of the network which each generate a _feature map_, hence an increase in data created by these convolutional filters...\n",
    "\n",
    "#### Examples of feature extraction with known convolutional filters (filter from [Prewitt](https://fr.wikipedia.org/wiki/Filtre_de_Prewitt)):\n",
    "\n",
    "As an example, the figure below shows the 4 *features maps* obtained by convolving a MNIST image (the number 7) with 4 3x3 filters well known in image processing (Prewitt filters for contour extraction ):\n",
    "\n",
    "<p style=\"text-align:center; font-style:italic; font-size:12px;\">\n",
    "    <img src=\"img/7_mnist_4_filters.png\" width=\"500\"><br>\n",
    "    [image credit: JLC]\n",
    "</p>\n",
    "\n",
    "We see that these filters act as edge detection filters: in the output images, the whitest pixels constitute what the filters detected:\n",
    "- filters (a) and (c) detect lower and upper horizontal contours,\n",
    "- filters (b) and (d) detect right and left vertical contours.\n",
    "\n",
    "These very simple examples allow you to understand how the extraction of *features* from an image by convolutional filtering works. In convolutional networks the values of the elements of the convolution filters are learned by the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General case: RGB images processed by several convolution filters\n",
    "\n",
    "In the general case where the images correspond to 3D arrays (the third dimension being for the 3 colors R(ed), G(reeen) & B(lue)), the convolution filter is also a 3D array. The operation remains identical to the 1D case: for a position of the 3D filter on the image, the contracted tensor product of the filter with the corresponding 3D sub-array in the image provides a scalar number, and the sweep of the process over the whole image gives the *feature map*.\n",
    "\n",
    "For example, if we use 10 5x5 convolution filters (10 arrays of dimensions (5,5,3)) to process (with _padding_) an RGB image of 32x32 pixels (array of dimensions (32,32,3), we obtains a *feature maps* of dimensions (32,32,10), i.e. 10240 pixels  whereas the source image only has 3072!\n",
    "\n",
    "<p style=\"text-align:center; font-style:italic; font-size:12px;\">\n",
    "    <img src=\"img/conv_3D_10.png\" width=\"350\"><br>\n",
    "    [image credit: <a href=\"https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2\">Arden Dertat</a>]\n",
    "</p>\n",
    "\n",
    "$\\leadsto$ To reduce the amount of information generated by convolution filters without losing too much information, convolution is always followed by a *pooling* operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the convolutional filter to the layer of convolutional neurons\n",
    "\n",
    "The integration of convolutional filtering in the structure of the neural network gives the following organization of the calculations:\n",
    "\n",
    "- Each convolutional filter has the same coefficients for the 3 colors: for the LeNet5 network for example, each of the 6 5x5x3 filters of the first layer has only 25 coefficients, identical for the R, G & B colors.\n",
    "\n",
    "- Each unit (convolutional neuron) of a *feature map* of layer C1 receives 75 pixels (25 red pixels $R_i$, 25 green pixels $G_i$ and 25 blue pixels $B_i$) delimited by the position of the convolutional filter in the source image.\n",
    "\n",
    "- The neuron $k$ of a *feature map* computes an output $y_k = F_a(\\sum_{i=1}^{25}{\\omega_i(R_i + G_i + B_i) - b_k})$, where $ b_k$ is the bias of the neuron $k$ and $F_a$ the activation function (very often `relu`).\n",
    "\n",
    "- for the 6 convolutional filters of layer C1, there are therefore 6 x (25 + 1) parameters = 156 unknown parameters for this layer which will be determined by network training.\n",
    "\n",
    "The same scheme is used in all the convoltional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Pooling*\n",
    "\n",
    "*Pooling* aims to reduce the amount of data to be processed. As for the convolution operation, we move a filter over the elements of the *feature map* array and at each position of the filter on the array, we calculate a number representing all the elements selected in the filter (for example the maximum value, or average...). But unlike convolution, we move the filter without overlapping.<br>\n",
    "In the simplified example below, the *max spool* filter transforms the 8x8 matrix into a 4x4 matrix that describes \"almost\" the same information but with less data:\n",
    "<p style=\"text-align:center; font-style:italic; font-size:12px;\">\n",
    "     <img src=\"img/max_pool_2x2.png\" width=\"350\"><br>\n",
    "     [image credit: JLC</a> ]\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work to do\n",
    "### 1 - Load and shape the MNIST data<br>2 - Build the Convolutional Neural Network<br>3 - Train the network with test at every *epoch*<br>4 - Run the network with the  MNIST data set test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Check Python modules\n",
    "The **keras** module which allows high-level manipulation of **tensorflow** objects is integrated in the **tensorflow** (tf) module since version 2. <br>\n",
    "The **tf.keras** module documentation to consult for this APP is here: [www.tensorflow.org/api_docs/python/tf/keras](https://www.tensorflow.org/api_docs/python/tf/keras)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete the (numerous) warning messages from the **tensorflow** module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import sys, cv2\n",
    "import matplotlib.pyplot as plt\n",
    "print(f\"Python    : {sys.version.split()[0]}\")\n",
    "print(f\"tensorflow: {tf.__version__} incluant keras {keras.__version__}\")\n",
    "print(f\"numpy     : {np.__version__}\")\n",
    "print(f\"OpenCV    : {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding matplotlib plots in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load and pre-process MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work of loading MNIST images has been explained in the *notebook* `ML1_MNIST.ipynb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST data set:\n",
    "(im_train, lab_train), (im_test, lab_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Define parameters :\n",
    "nb_im_train = im_train.shape[0]    # number of train images\n",
    "nb_im_test  = im_test.shape[0]     # number of test images\n",
    "nb_pixel    = im_test[0].size      # number of pixels par iamge\n",
    "nb_classe   = len(set(lab_test))   # number of classes (10 digits from 0 to 9)\n",
    "\n",
    "print(f\"{nb_im_train} train images and {nb_im_test} test images\")\n",
    "print(f\"{nb_im_test} test images\")\n",
    "print(f\"{nb_pixel} pixels in each image\")\n",
    "print(f\"{nb_classe} classes (the digits from 0 to 9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional layers of the *kera*s module expect 4-dimensional arrays `(batch_size, height, width, depth)` by default:\n",
    "- `batch_size`: number of input images,\n",
    "- `height` and `width`: height and width of images (in pixels),\n",
    "- `depth`: depth of the arrays (`3` for an RGB image, `1` for a grayscale image).\n",
    "\n",
    "The form of the MNIST images is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_train.shape, im_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to add a dimension (equal to `1`) after the third dimension `28`, for example with the `reshape` method of the `ndarray` arrays of numpy.\n",
    "\n",
    "Complete the following cell to define `x_train` and `x_test` obtained:\n",
    "- by adding a fourth dimension equal to 1 to the `im_train` and `im_test` arrays\n",
    "- and by normalizing the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the reshape methode of ndarray:\n",
    "\n",
    "...to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking: :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_train.shape, x_train.shape, im_test.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *one-hot* formatting of MNIST labels\n",
    "\n",
    "The work of transforming the MNIST labels into *one-hot* vectors has been covered in the *notebook* `ML2_DNN.ipynb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# 'one-hot' encoding of labels :\n",
    "y_train = to_categorical(lab_train)\n",
    "y_test  = to_categorical(lab_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Build the Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the **convolutional** neural network in the cell below using the **keras** module.\n",
    "\n",
    "As for the dense network, we create an object `model` instance of the class `Sequential` (cf [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) ), then we complete `model` incrementally by adding each layer with the `add` method:\n",
    "\n",
    "- The input layer of type `InputLayer` (cf [tf.keras.layers.InputLayer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/InputLayer)) is used to specify the form of the input data.<br>\n",
    "The shape expected by keras for input images is (height, width, depth): it can be obtained for example with the `shape` attribute of any image in the `x_train` array.<br><br >\n",
    "\n",
    "- The convolutional layers are of the `Conv2D` type (cf [tf.keras.layers.Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)):\n",
    "    - the first 2 positional arguments are:\n",
    "        - the number of filters in the layer\n",
    "        - the form of the filter: you can specify `N` or `(N,N)` to specify an N x N filter\n",
    "    - the other named arguments used are:\n",
    "        - `stride`: the step of the displacement of the convolution filter, default value: `stride=1` (equivalent to `(1, 1)`))\n",
    "        - `padding=valid`: no padding, or `padding=same`: output of same size as input (default: `valid`)\n",
    "        - `activation`: choice of the activation function (`'relu'`, '`tanh'`...)<br><br>\n",
    "        \n",
    "- The *pooling* layers of the historic LeNet5 network use an *average pool* filter which corresponds to the class `AveragePooling2D` (cf [tf.keras.layers.AveragePooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D)), but we will have better results with a *max pool* filtering which retains the max value of the pixels in the filter window (see the page [tf.keras.layers.MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)). Main arguments to use with `MaxPool2D`:\n",
    "    - `pool_size`: `N` or `(N,N)` to specify an N x N filter (default: `(2,2)`)\n",
    "    - `strides`: int, tuple of 2 int, or None. If None (default), takes the same value as `pool_size`\n",
    "    - `padding`: as for class `Conv2D`<br><br>\n",
    "\n",
    "- To flatten the 16 *feature maps* 5x5 into a vector of 16 * 5 * 5 = 635 elements, we can use a `Flatten` layer (cf [tf.keras.layers.Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten))<br><br>\n",
    "\n",
    "Inspired by the structure of the `LeNet5` network and the specifications above, we obtain:\n",
    "- input layer: `Input(shape=x_train[0].shape)`: we use the `shape` attribute of the 1st image which is worth (28,28,1).\n",
    "- layer C1: `Conv2D(6, 5, padding='same', activation='relu', name='C1')`\n",
    "- S2 layer: `MaxPool2D(pool_size=2, name='S2')`\n",
    "- layer C3: `Conv2D(16, 5, padding='valid', activation='relu', name='C3')`\n",
    "- S4 layer: `MaxPool2D(pool_size=2, name='S4')`\n",
    "- application layer: `Flatten()`\n",
    "- layer C5: `Dense(200, activation='relu', name='C5')`\n",
    "- layer F5: `Dense(84, activation='relu', name='F6'`\n",
    "- OUTPUT layer: `Dense(nb_classe, activation='softmax', name='Output')`\n",
    "\n",
    "Once built, the network must be compiled (in the sense of tensorflow) with the `compile` method using for example the arguments:\n",
    "- `loss='categorical_crossentropy'`: choice of the error function (cf [tf.keras.categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy))\n",
    "- `optimizer='adam'`: choice of Adam optimizer (see page [tf.keras.optimizers.Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) )\n",
    "- `metrics=['accuracy']` to obtain the data used to plot the performance curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D,  MaxPool2D, Flatten\n",
    "\n",
    "# fixer la graine des générateurs aléatoires utilisés par tensorflow:\n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "model = Sequential(name='lenet')\n",
    "model.add(Input(shape=x_train[0].shape))\n",
    "model.add(Conv2D(6, 5, padding='same', activation='relu', name='C1'))\n",
    "model.add(MaxPool2D(pool_size=2, name='S2'))\n",
    "model.add(Conv2D(16, 5, padding='valid', activation='relu', name='C3'))\n",
    "model.add(MaxPool2D(pool_size=2, name='S4'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200, activation='relu', name='C5'))\n",
    "model.add(Dense(84, activation='relu', name='F6'))\n",
    "model.add(Dense(nb_classe, activation='softmax', name='Output'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `summary` method of the `model` object, display the description of the model: note the values of the parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.keras.utils.plot_model` function also allows to draw the structure of the network (see the page \n",
    "[tf.keras.utils.plot_model](https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model)).<br>\n",
    "Trace the model structure by adding the `show_shapes=True` option to the `model` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the initial state of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the initial state of the weights of the untrained network (random values) with the `Model.save_weights` method. <br>\n",
    "This will be useful later to reset the network to its initial state before restarting other trainings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check whether the folder 'weights' exists and cretae it if needed:\n",
    "if not os.path.isdir(\"weights\"): os.mkdir(\"weights\")\n",
    "\n",
    "# Save the initial DNN (random) weights:\n",
    "key = 'conv1_init'\n",
    "model.save_weights(os.path.join('weights', key))\n",
    "\n",
    "# Display the created files:\n",
    "files=[os.path.join(\"weights\",f) for f in os.listdir(\"weights\") if f.startswith(key)]\n",
    "for f in files: print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the `save_weights` method uses the `key` argument to prefix the created files.<br>\n",
    "When loading the NDD weights later with the `load_weights` method of the `Sequential` class, just give the same key to retrieve the relevant files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Train the network with evaluation at each *epoch*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, consult the documentation of the `fit` method on the page [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential).\n",
    "\n",
    "Complete the cell below to train the CNN with the `fit` method of the `model` object using the arguments:\n",
    "- `x_train`: the 60000 images\n",
    "- `y_train`: the 60000 *one-hot* encoded labels.\n",
    "- `epochs=10`: repeat full training 10 times.\n",
    "- `batch_size=64`: split the input data set (the 60000 images) into \"batches\" of size `batch_size` (here: batches of 64 images).<br>\n",
    "Updating network weights is done after batches of `batch_size` images.<br>\n",
    "The value of `batch_size` (by default: 32) is a parameter that influences the quality of the training but also its memory footprint: you can later try different values (64, 128, 256 ...) and observe how the quality of the training evolves)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better indicator of the quality of the trained network, you can test at each `epoch` the precision of the inferences of the trained network using the test data: just pass the `validation_data` argument to the `fit` method, assigning it the test data tuple `(x_test, y_test)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the initial state of the DNN\n",
    "key = 'conv1_init'\n",
    "model.load_weights(os.path.join('weights', key))\n",
    "\n",
    "# set the seed of the random generators inolved by tensorflow:\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 batch_size=64,\n",
    "                 epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hist` object returned by the `fit` method has a dictionary-type `history` attribute whose keys `'loss'`, `'accuracy'` contain the evaluation of the cost function and the accuracy of the network at the end of each (*epoch*) with training data. The `'val_loss'` and `'val_accuracy'` keys are associated with test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist.history['loss'])\n",
    "print(hist.history['accuracy'])\n",
    "print(hist.history['val_loss'])\n",
    "print(hist.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of `accuracy` and `loss` curves of training and testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot_loss_accuracy` function of the `utils.tools` module (found in the notebook directory) plots accuracy and loss curves using the data stored in the `hist` object. Plot these curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Train the network whith evaluation at each *epoch* and management of the *over-fit*\n",
    "\n",
    "The `Keras` module offers tools to automatically stop the training by monitoring, for example, the growth of precision from one `epoch` to another.\n",
    "You define the parameters of the `EarlyStopping` (cf [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)) *callback* and pass it to the method `fit` via the `callbacks` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "callbacks_list = [ \n",
    "    EarlyStopping(monitor='val_accuracy',  # the parameter to monitor\n",
    "                  mode='max',              # the parameter is suppose to increse\n",
    "                  patience=2,              # accept that the parameter decreases twice\n",
    "                  restore_best_weights=True,\n",
    "                  verbose=1)\n",
    "]\n",
    "\n",
    "# relod the DNN initial state:\n",
    "key = 'conv1_init'\n",
    "model.load_weights(os.path.join('weights', key))\n",
    "\n",
    "# set the seed of the random generators inolved by tensorflow:\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    epochs=15, \n",
    "                    batch_size=68, \n",
    "                    callbacks = callbacks_list)\n",
    "\n",
    "from utils.tools import plot_loss_accuracy\n",
    "plot_loss_accuracy(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Save the trained CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **weights** or **the structure and weights** of a trained network can be saved in a file with the `save_weights` and `save` methods of the `Sequential` class.<br><br>\n",
    "\n",
    "### Save the weights of the trained CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Check whether the folder 'weights' exists and create it if needed:\n",
    "if not os.path.exists(\"weights\"): os.mkdir(\"weights\")\n",
    "\n",
    "# save the trained CNN weights:\n",
    "key = 'conv1_trained'\n",
    "model.save_weights(os.path.join('weights', key))\n",
    "\n",
    "# Display the created files:\n",
    "files=[os.path.join(\"weights\",f) for f in os.listdir(\"weights\") if f.startswith(key)]\n",
    "for f in files: print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the weights AND structure of the trained CNN\n",
    "\n",
    "The `save` method of the `Sequential` class saves **the structure** and the **weights** of the trained DNN<br>\n",
    "You can use the `tf.keras.models.load_model` function to re-create the network later and reload its trained weights to exploit it in operational situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Check whether the folder 'models' exists and create it if needed:\n",
    "if not os.path.exists(\"models\"): os.mkdir(\"models\")\n",
    "\n",
    "# save the trained DNN structure + wieghts:\n",
    "key = 'conv1_trained'\n",
    "model.save(os.path.join('models', key) )\n",
    "\n",
    "# Display the created files:\n",
    "files=[os.path.join(\"models\",f) for f in os.listdir(\"models\") if f.startswith(key)]\n",
    "for f in files: print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional network tends towards a better accuracy close to 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Exploiting the trained network: `predict` method\n",
    "\n",
    "The `predict` method is used to compute the CNN inferences for one or more inputs (see the `predict` method in the page \n",
    "[tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#predict)).\n",
    "\n",
    "The cell below shows an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100  # number of the test image \n",
    "\n",
    "# display the image:\n",
    "from utils.tools import plot_images\n",
    "plot_images(im_test,i,1,1) ; plt.show()\n",
    "\n",
    "# compute the trained DNN inference inférence for tes test image:\n",
    "rep = model.predict(x_test[i:i+1])      # Warning: x must be an array of matrixes, not a simple matrix\n",
    "                                        # => x[i] does not work!\n",
    "\n",
    "print(f\"CNN inférence for the test image #{i} :\\n{rep[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to make the output of the network more readable, we can limit the display of the numpy array to 2 decimal places:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(formatter={'float':'{:.2f}'.format}):    \n",
    "    print(f\"CNN inférence for the test image #{i} rounded to 2 digits: {rep[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `argmax` method of the *ndarray* arrays of *numpy* gives the rank of the maximum value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predicted label is rep[0].argmax(): {rep[0].argmax()}\")\n",
    "print(f\"True label of the test image #{i} : {lab_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usefulness of numpy's `argmax` method to decode the array of *one-hot* vectors returned by `predict`\n",
    "\n",
    "When you compute inferences of the CNN for the images of the `x_test` array for example, you get an array of *one-hot* vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(x_test)\n",
    "print(\"shape of the 'results' ndarray:\", results.shape)\n",
    "print(\"Example of display of vectors in the 'result' ndarray:\")\n",
    "with np.printoptions(formatter={'float':'{:.2f}'.format}): \n",
    "    print(\"\\tresults[0]  :\", results[0])\n",
    "    print(\"\\tresults[-1] :\", results[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the expression `results.argmax(axe=-1)`, you get the array of the `argmax` of each vector $\\leadsto$ it is the array of the digits classified by the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferences = results.argmax(axis=-1)\n",
    "print(f\"inferences.shape: {inferences.shape}, inferences.dtype: {inferences.dtype}\")\n",
    "print(f\"Content of 'inferences': {inferences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare `inferences` and `lab_test` with the `==` operator (this makes sense with the *ndarray* of the *numpy* module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferences == lab_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by counting the number of `True` we get the number of correct inferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_ok = (inferences == lab_test)\n",
    "print(f\"number of true inferences: {inference_ok.sum()} over {nb_im_test} test images\")\n",
    "\n",
    "precision = inference_ok.sum()/nb_im_test*100\n",
    "print(f\"precision of the trained DNN: {precision:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Show Confusion Matrix\n",
    "\n",
    "The `show_cm` function from the `utils.tools` module displays the **confusion matrix** to visualize:\n",
    "- on the diagonal: the correct inferences of the NN, with the number of correct answers in each box\n",
    "- off diagonal: the NN errors, with in each box the number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import show_cm\n",
    "help(show_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `show_cm` function with arguments `lab_test`, `results` and the list of classes to display the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other interesting resources... videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/aircAruvnKk\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/aircAruvnKk\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/IHZwWFHWa-w\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/IHZwWFHWa-w\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/Ilg3gGewQ5U\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/Ilg3gGewQ5U\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
